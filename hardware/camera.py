import cv2
import numpy as np
import requests
import time
import mediapipe as mp
from face.recognition import recognize_face
from voice.recognition import recognize_speech
from voice.synthesis import speak_async
from face.training import learn_face
from hardware.esp32_control import send_command_to_esp, turn_on_light, turn_off_light
from hardware.gestures import init_hand_detector, detect_hand_skeleton, get_hand_gesture, get_finger_direction, \
    draw_hand_skeleton


AUTO_LIGHT_ENABLED = False
_LAST_LIGHT_STATE = None
_BRIGHT_LOW = 70
_BRIGHT_HIGH = 110


def set_auto_light(enabled: bool) -> bool:
    global AUTO_LIGHT_ENABLED
    AUTO_LIGHT_ENABLED = bool(enabled)
    try:
        speak_async(f"–ê–≤—Ç–æ–º–∞—Ç–∏—á–Ω–µ —Å–≤—ñ—Ç–ª–æ {'—É–≤—ñ–º–∫–Ω–µ–Ω–æ' if AUTO_LIGHT_ENABLED else '–≤–∏–º–∫–Ω–µ–Ω–æ'}.")
    except Exception:
        pass
    print(f"[AUTO-LIGHT] Enabled = {AUTO_LIGHT_ENABLED}")
    return AUTO_LIGHT_ENABLED


def toggle_auto_light() -> bool:
    return set_auto_light(not AUTO_LIGHT_ENABLED)


def _maybe_adjust_light(frame_gray: np.ndarray):
    global _LAST_LIGHT_STATE
    if not AUTO_LIGHT_ENABLED:
        return

    try:
        mean_luma = float(frame_gray.mean())
    except Exception:
        return

    if _LAST_LIGHT_STATE is None:
        _LAST_LIGHT_STATE = 'on' if mean_luma < (_BRIGHT_LOW + _BRIGHT_HIGH) / 2 else 'off'

    if _LAST_LIGHT_STATE == 'off' and mean_luma < _BRIGHT_LOW:
        try:
            turn_on_light()
            _LAST_LIGHT_STATE = 'on'
            print(f"[AUTO-LIGHT] ON (mean={mean_luma:.1f})")
        except Exception as e:
            print(f"[AUTO-LIGHT] ON failed: {e}")
    elif _LAST_LIGHT_STATE == 'on' and mean_luma > _BRIGHT_HIGH:
        try:
            turn_off_light()
            _LAST_LIGHT_STATE = 'off'
            print(f"[AUTO-LIGHT] OFF (mean={mean_luma:.1f})")
        except Exception as e:
            print(f"[AUTO-LIGHT] OFF failed: {e}")


def start_camera_tracking_with_recognition():

    print("üì∑ –ó–ê–ü–£–°–ö –ö–ê–ú–ï–†–ò –ó –†–û–ó–ü–Ü–ó–ù–ê–í–ê–ù–ù–Ø–ú")

    esp32cam_url = "http://192.168.4.1/capture"
    face_cascade = cv2.CascadeClassifier(cv2.data.haarcascades + 'haarcascade_frontalface_default.xml')

    hand_detector = init_hand_detector()

    camera_active = True
    last_unknown_face_time = 0
    learn_offered = False

    try:
        while camera_active:
            response = requests.get(esp32cam_url, timeout=2)
            if response.status_code == 200:
                img_array = np.array(bytearray(response.content), dtype=np.uint8)
                frame = cv2.imdecode(img_array, cv2.IMREAD_COLOR)
                frame = cv2.rotate(frame, cv2.ROTATE_90_COUNTERCLOCKWISE)
                frame = cv2.resize(frame, (480, 640))
                gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)
                _maybe_adjust_light(gray)

                faces = face_cascade.detectMultiScale(gray, 1.3, 5)
                face_detected = False
                known_face_detected = False
                current_face_name = None

                for (x, y, w, h) in faces:
                    face_detected = True
                    cv2.rectangle(frame, (x, y), (x + w, y + h), (0, 255, 0), 2)

                    face_roi = frame[y:y + h, x:x + w]
                    name = recognize_face(face_roi)

                    if name:
                        known_face_detected = True
                        current_face_name = name
                        cv2.putText(frame, f"–í—ñ—Ç–∞—é, {name}!", (x, y - 10),
                                    cv2.FONT_HERSHEY_SIMPLEX, 0.7, (0, 255, 0), 2)
                        learn_offered = False  # –°–∫–∏–¥–∞—î–º–æ –ø—Ä–æ–ø–æ–∑–∏—Ü—ñ—é –Ω–∞–≤—á–∞–Ω–Ω—è
                    else:
                        cv2.putText(frame, "–ù–µ–≤—ñ–¥–æ–º–µ –æ–±–ª–∏—á—á—è", (x, y - 10),
                                    cv2.FONT_HERSHEY_SIMPLEX, 0.7, (0, 0, 255), 2)

                if face_detected and not known_face_detected and not learn_offered:
                    current_time = time.time()
                    if current_time - last_unknown_face_time > 5:  # –ß–µ–∫–∞—î–º–æ 5 —Å–µ–∫—É–Ω–¥
                        last_unknown_face_time = current_time
                        learn_offered = True
                        print("üë§ –ù–ï–í–Ü–î–û–ú–ï –û–ë–õ–ò–ß–ß–Ø - –ø—Ä–æ–ø–æ–Ω—É—é –Ω–∞–≤—á–∞–Ω–Ω—è")
                        send_command_to_esp("stop")
                        cv2.putText(frame, "–ù–æ–≤–µ –æ–±–ª–∏—á—á—è! –ü—Ä–æ–ø–æ–Ω—É—é –Ω–∞–≤—á–∞–Ω–Ω—è...", (50, 300),
                                    cv2.FONT_HERSHEY_SIMPLEX, 0.7, (0, 255, 255), 2)
                        cv2.imshow('–ö–∞–º–µ—Ä–∞ - –í—ñ–¥—Å—Ç–µ–∂–µ–Ω–Ω—è –æ–±–ª–∏—á —Ç–∞ —Ä—É–∫', frame)
                        cv2.waitKey(1)

                        speak_async(
                            "–Ø –±–∞—á—É –Ω–æ–≤–µ –æ–±–ª–∏—á—á—è. –•–æ—á–µ—Ç–µ, —â–æ–± —è –Ω–∞–≤—á–∏–≤—Å—è –π–æ–≥–æ —Ä–æ–∑–ø—ñ–∑–Ω–∞–≤–∞—Ç–∏? –°–∫–∞–∂—ñ—Ç—å '—Ç–∞–∫' –∞–±–æ '–Ω—ñ'.")


                        response = recognize_speech(timeout=10)
                        if response and "—Ç–∞–∫" in response.lower():
                            print("‚úÖ –ö–û–†–ò–°–¢–£–í–ê–ß –ü–û–ì–û–î–ò–í–°–Ø –ù–ê –ù–ê–í–ß–ê–ù–ù–Ø")
                            speak_async("–ß—É–¥–æ–≤–æ! –ü–æ—á–∏–Ω–∞—é –Ω–∞–≤—á–∞–Ω–Ω—è.")
                            cv2.destroyAllWindows()
                            learn_face()
                            return
                        elif response and "–Ω—ñ" in response.lower():
                            print("‚ùå –ö–û–†–ò–°–¢–£–í–ê–ß –í–Ü–î–ú–û–í–ò–í–°–Ø –í–Ü–î –ù–ê–í–ß–ê–ù–ù–Ø")
                            speak_async("–î–æ–±—Ä–µ, –Ω–µ –±—É–¥—É –Ω–∞–≤—á–∞—Ç–∏—Å—è.")
                        else:
                            print("‚ùå –ù–ï –ó–†–û–ó–£–ú–Ü–õ–ê –í–Ü–î–ü–û–í–Ü–î–¨")
                            speak_async("–ù–µ –∑—Ä–æ–∑—É–º—ñ–≤ –≤–∞—à—É –≤—ñ–¥–ø–æ–≤—ñ–¥—å.")


                hand_landmarks, handedness = detect_hand_skeleton(frame, hand_detector)
                if hand_landmarks:
                    gesture, fingers_up = get_hand_gesture(hand_landmarks, frame.shape[1], frame.shape[0])
                    direction, angle = get_finger_direction(hand_landmarks, frame.shape[1], frame.shape[0])


                    if gesture == "fist":
                        send_command_to_esp("forward")
                        status_text = "–á–¥—É –≤–ø–µ—Ä–µ–¥ - –∫—É–ª–∞–∫"
                    elif gesture == "open_hand":
                        send_command_to_esp("stop")
                        status_text = "–°—Ç–æ—é - –≤—ñ–¥–∫—Ä–∏—Ç–∞ —Ä—É–∫–∞"
                    elif gesture == "one_finger":
                        if direction == "left":
                            send_command_to_esp("left")
                            status_text = "–ü–æ–≤–æ—Ä–æ—Ç –ª—ñ–≤–æ—Ä—É—á - –≤–∫–∞–∑—ñ–≤–Ω–∏–π –ø–∞–ª–µ—Ü—å"
                        elif direction == "right":
                            send_command_to_esp("right")
                            status_text = "–ü–æ–≤–æ—Ä–æ—Ç –ø—Ä–∞–≤–æ—Ä—É—á - –≤–∫–∞–∑—ñ–≤–Ω–∏–π –ø–∞–ª–µ—Ü—å"
                        else:
                            send_command_to_esp("forward")
                            status_text = "–á–¥—É –≤–ø–µ—Ä–µ–¥ - –≤–∫–∞–∑—ñ–≤–Ω–∏–π –ø–∞–ª–µ—Ü—å"
                    else:
                        send_command_to_esp("stop")
                        status_text = "–°—Ç–æ—é - —ñ–Ω—à–∏–π –∂–µ—Å—Ç"

                    frame = draw_hand_skeleton(frame, hand_landmarks, gesture, direction)
                    cv2.putText(frame, status_text, (10, frame.shape[0] - 30),
                                cv2.FONT_HERSHEY_SIMPLEX, 0.5, (255, 255, 255), 1)
                else:
                    send_command_to_esp("stop")
                    cv2.putText(frame, "–†—É–∫–∞ –Ω–µ –∑–Ω–∞–π–¥–µ–Ω–∞", (10, frame.shape[0] - 30),
                                cv2.FONT_HERSHEY_SIMPLEX, 0.5, (255, 255, 255), 1)

                cv2.putText(frame, "–†–µ–∂–∏–º –∫–∞–º–µ—Ä–∏: –í—ñ–¥—Å—Ç–µ–∂–µ–Ω–Ω—è –æ–±–ª–∏—á —Ç–∞ —Ä—É–∫", (10, 30),
                            cv2.FONT_HERSHEY_SIMPLEX, 0.6, (255, 255, 255), 2)

                cv2.putText(frame, f"Auto-light: {'ON' if AUTO_LIGHT_ENABLED else 'OFF'}", (10, 55),
                            cv2.FONT_HERSHEY_SIMPLEX, 0.6, (255, 255, 255), 2)

                if current_face_name:
                    cv2.putText(frame, f"–†–æ–∑–ø—ñ–∑–Ω–∞–Ω–æ: {current_face_name}", (10, 80),
                                cv2.FONT_HERSHEY_SIMPLEX, 0.6, (0, 255, 0), 2)
                else:
                    cv2.putText(frame, "–û–±–ª–∏—á—á—è: –Ω–µ —Ä–æ–∑–ø—ñ–∑–Ω–∞–Ω–æ", (10, 80),
                                cv2.FONT_HERSHEY_SIMPLEX, 0.6, (0, 0, 255), 2)

                cv2.putText(frame, "–°–∫–∞–∂—ñ—Ç—å '—Å—Ç–æ–ø' –¥–ª—è –≤–∏—Ö–æ–¥—É", (10, 105),
                            cv2.FONT_HERSHEY_SIMPLEX, 0.5, (255, 255, 255), 1)

                cv2.imshow('–ö–∞–º–µ—Ä–∞ - –í—ñ–¥—Å—Ç–µ–∂–µ–Ω–Ω—è –æ–±–ª–∏—á —Ç–∞ —Ä—É–∫', frame)
            try:
                voice_command = recognize_speech(timeout=1)
                if voice_command and "—Å—Ç–æ–ø" in voice_command.lower():
                    print("üì∑ –ö–û–ú–ê–ù–î–ê –í–ò–•–û–î–£ –ó –ö–ê–ú–ï–†–ò")
                    camera_active = False
                    speak_async("–í–∏–º–∏–∫–∞—é –∫–∞–º–µ—Ä—É.")
                    break
            except:
                pass

            if cv2.waitKey(1) & 0xFF == ord('q'):
                camera_active = False
                break

            time.sleep(0.1)

    except Exception as e:
        print(f"‚ùå –ü–û–ú–ò–õ–ö–ê –ö–ê–ú–ï–†–ò: {e}")
        speak_async("–ü–æ–º–∏–ª–∫–∞ —Ä–æ–±–æ—Ç–∏ –∫–∞–º–µ—Ä–∏.")

    finally:
        cv2.destroyAllWindows()
        send_command_to_esp("stop")
        print("üì∑ –ö–ê–ú–ï–†–ê –í–ò–ú–ö–ù–ï–ù–ê")
