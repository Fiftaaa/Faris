import cv2
import numpy as np
import requests
import time
import datetime
from sklearn.neighbors import KNeighborsClassifier
from sklearn.preprocessing import LabelEncoder
from face.recognition import extract_face_features, load_face_data, save_face_data
from voice.recognition import recognize_speech
from voice.synthesis import speak_async
from utils.config import ESP32_CAM_URL


def learn_face():
    print("üë§ –ü–û–ß–ê–¢–û–ö –ù–ê–í–ß–ê–ù–ù–Ø –û–ë–õ–ò–ß–ß–Ø")
    speak_async("–†–µ–∂–∏–º –Ω–∞–≤—á–∞–Ω–Ω—è –æ–±–ª–∏—á—á—è. –ë—É–¥—å –ª–∞—Å–∫–∞, —Å–∫–∞–∂—ñ—Ç—å –≤–∞—à–µ —ñ–º'—è.")

    name = recognize_speech(timeout=10)
    if not name:
        print("‚ùå –ù–µ –≤–¥–∞–ª–æ—Å—è —Ä–æ–∑–ø—ñ–∑–Ω–∞—Ç–∏ —ñ–º'—è")
        speak_async("–ù–µ –≤–¥–∞–ª–æ—Å—è —Ä–æ–∑–ø—ñ–∑–Ω–∞—Ç–∏ —ñ–º'—è. –°–ø—Ä–æ–±—É–π—Ç–µ —â–µ —Ä–∞–∑.")
        return False

    name = name.strip().capitalize()
    print(f"üë§ –Ü–ú'–Ø –î–õ–Ø –ù–ê–í–ß–ê–ù–ù–Ø: '{name}'")
    speak_async(f"–ù–∞–≤—á–∞—é—Å—è —Ä–æ–∑–ø—ñ–∑–Ω–∞–≤–∞—Ç–∏ {name}. –ü–æ–∫–∞–∂—ñ—Ç—å —Å–≤–æ—î –æ–±–ª–∏—á—á—è –≤ –∫–∞–º–µ—Ä—É. –ú–µ–Ω—ñ –ø–æ—Ç—Ä—ñ–±–Ω–æ 20 –∑–Ω—ñ–º–∫—ñ–≤.")

    esp32cam_url = ESP32_CAM_URL
    face_cascade = cv2.CascadeClassifier(cv2.data.haarcascades + 'haarcascade_frontalface_default.xml')
    features_list = []
    captured_count = 0

    try:
        # –ó–∞–≤–∞–Ω—Ç–∞–∂—É—î–º–æ —ñ—Å–Ω—É—é—á—ñ –¥–∞–Ω—ñ —Ä–æ–∑–ø—ñ–∑–Ω–∞–≤–∞–Ω–Ω—è –æ–±–ª–∏—á
        classifier, le = load_face_data()
        if classifier is None or le is None:
            print("üîß –°—Ç–≤–æ—Ä—é—é –Ω–æ–≤—É –º–æ–¥–µ–ª—å —Ä–æ–∑–ø—ñ–∑–Ω–∞–≤–∞–Ω–Ω—è –æ–±–ª–∏—á")
            classifier = KNeighborsClassifier(n_neighbors=3)
            le = LabelEncoder()


        print("üì∏ –ó–±—ñ—Ä –∑–Ω—ñ–º–∫—ñ–≤ –¥–ª—è –Ω–∞–≤—á–∞–Ω–Ω—è...")
        while captured_count < 20:
            response = requests.get(esp32cam_url, timeout=2)
            if response.status_code == 200:
                img_array = np.array(bytearray(response.content), dtype=np.uint8)
                frame = cv2.imdecode(img_array, cv2.IMREAD_COLOR)
                frame = cv2.resize(frame, (640, 480))

                gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)
                faces = face_cascade.detectMultiScale(gray, 1.3, 5)

                for (x, y, w, h) in faces:
                    face_roi = frame[y:y + h, x:x + w]

                    features = extract_face_features(face_roi)
                    if features is not None:
                        features_list.append(features)
                        captured_count += 1
                        print(f"üì∏ –ó–Ω—ñ–º–æ–∫ {captured_count}/20 —É—Å–ø—ñ—à–Ω–æ –∑–∞—Ö–æ–ø–ª–µ–Ω–∏–π")

                    cv2.rectangle(frame, (x, y), (x + w, y + h), (0, 255, 0), 2)
                    cv2.putText(frame, f"–ù–∞–≤—á–∞–Ω–Ω—è: {name}", (10, 30),
                                cv2.FONT_HERSHEY_SIMPLEX, 0.7, (0, 255, 0), 2)
                    cv2.putText(frame, f"–ó–Ω—ñ–º–∫—ñ–≤: {captured_count}/20", (10, 60),
                                cv2.FONT_HERSHEY_SIMPLEX, 0.6, (0, 255, 0), 2)
                    cv2.putText(frame, "–î–∏–≤—ñ—Ç—å—Å—è –≤ –∫–∞–º–µ—Ä—É", (10, 90),
                                cv2.FONT_HERSHEY_SIMPLEX, 0.5, (255, 255, 255), 1)
                cv2.imshow('–ù–∞–≤—á–∞–Ω–Ω—è –æ–±–ª–∏—á—á—è', frame)
                cv2.waitKey(1)

            time.sleep(0.3)

        print("üß† –ù–∞–≤—á–∞–Ω–Ω—è –º–æ–¥–µ–ª—ñ —Ä–æ–∑–ø—ñ–∑–Ω–∞–≤–∞–Ω–Ω—è...")
        if features_list:
            if hasattr(le, 'classes_'):
                existing_classes = list(le.classes_)
                if name not in existing_classes:
                    existing_classes.append(name)
                    le.fit(existing_classes)
            else:
                le.fit([name])

            encoded_labels = le.transform([name] * len(features_list))
            classifier.fit(features_list, encoded_labels)

            if save_face_data(classifier, le):
                print("‚úÖ –ú–æ–¥–µ–ª—å –æ–±–ª–∏—á—á—è —É—Å–ø—ñ—à–Ω–æ –Ω–∞–≤—á–µ–Ω–∞ —Ç–∞ –∑–±–µ—Ä–µ–∂–µ–Ω–∞")

                speak_async(
                    f"–ß—É–¥–æ–≤–æ, {name}! –¢–µ–ø–µ—Ä —Ä–æ–∑–∫–∞–∂—ñ—Ç—å —Ç—Ä–æ—Ö–∏ –ø—Ä–æ —Å–µ–±–µ. –Ø–∫—ñ —É –≤–∞—Å —ñ–Ω—Ç–µ—Ä–µ—Å–∏, —Ö–æ–±—ñ —á–∏ —É–ª—é–±–ª–µ–Ω—ñ —Ç–µ–º–∏?")
                user_info = recognize_speech(timeout=15)

                if user_info:
                    from face.database import load_user_database, save_user_database

                    user_db = load_user_database()
                    user_db[name] = {
                        "interests": user_info,
                        "learned_date": datetime.datetime.now().strftime("%Y-%m-%d %H:%M:%S")
                    }

                    if save_user_database(user_db):
                        print(f"‚úÖ –Ü–Ω—Ñ–æ—Ä–º–∞—Ü—ñ—é –ø—Ä–æ {name} –∑–±–µ—Ä–µ–∂–µ–Ω–æ –≤ –±–∞–∑—ñ –¥–∞–Ω–∏—Ö")
                        speak_async(f"–î—è–∫—É—é! –Ø –∑–∞–ø–∞–º'—è—Ç–∞–≤ –≤–∞—à—É —ñ–Ω—Ñ–æ—Ä–º–∞—Ü—ñ—é. –†–∞–¥–∏–π –∑–Ω–∞–π–æ–º—Å—Ç–≤—É, {name}!")
                    else:
                        print(f"‚ùå –ù–µ –≤–¥–∞–ª–æ—Å—è –∑–±–µ—Ä–µ–≥—Ç–∏ —ñ–Ω—Ñ–æ—Ä–º–∞—Ü—ñ—é –ø—Ä–æ {name}")
                        speak_async(f"–Ü–Ω—Ñ–æ—Ä–º–∞—Ü—ñ—é –≤–¥–∞–ª–æ—Å—è —Ä–æ–∑–ø—ñ–∑–Ω–∞—Ç–∏, –∞–ª–µ –≤–∏–Ω–∏–∫–ª–∏ –ø—Ä–æ–±–ª–µ–º–∏ –∑—ñ –∑–±–µ—Ä–µ–∂–µ–Ω–Ω—è–º.")
                else:
                    print(f"‚ÑπÔ∏è –Ü–Ω—Ñ–æ—Ä–º–∞—Ü—ñ—é –ø—Ä–æ —ñ–Ω—Ç–µ—Ä–µ—Å–∏ –Ω–µ —Ä–æ–∑–ø—ñ–∑–Ω–∞–Ω–æ, –∑–±–µ—Ä—ñ–≥–∞—é –ª–∏—à–µ –æ–±–ª–∏—á—á—è")
                    speak_async(f"–Ü–Ω—Ñ–æ—Ä–º–∞—Ü—ñ—é –Ω–µ –≤–¥–∞–ª–æ—Å—è —Ä–æ–∑–ø—ñ–∑–Ω–∞—Ç–∏, –∞–ª–µ —è –∑–∞–ø–∞–º'—è—Ç–∞–≤ –≤–∞—à–µ –æ–±–ª–∏—á—á—è, {name}!")

                success_frame = np.zeros((200, 500, 3), dtype=np.uint8)
                cv2.putText(success_frame, "–ù–ê–í–ß–ê–ù–ù–Ø –£–°–ü–Ü–®–ù–ï!", (50, 80),
                            cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 255, 0), 2)
                cv2.putText(success_frame, f"–û–±–ª–∏—á—á—è {name} –¥–æ–¥–∞–Ω–æ", (50, 120),
                            cv2.FONT_HERSHEY_SIMPLEX, 0.7, (255, 255, 255), 2)
                cv2.imshow('–†–µ–∑—É–ª—å—Ç–∞—Ç', success_frame)
                cv2.waitKey(2000)

                cv2.destroyAllWindows()
                return True
            else:
                print("‚ùå –ü–æ–º–∏–ª–∫–∞ –∑–±–µ—Ä–µ–∂–µ–Ω–Ω—è –º–æ–¥–µ–ª—ñ –æ–±–ª–∏—á—á—è")
                speak_async("–ü–æ–º–∏–ª–∫–∞ –∑–±–µ—Ä–µ–∂–µ–Ω–Ω—è –¥–∞–Ω–∏—Ö.")
        else:
            print("‚ùå –ù–µ –≤–¥–∞–ª–æ—Å—è –∑–Ω–∞–π—Ç–∏ –æ–±–ª–∏—á—á—è –¥–ª—è –Ω–∞–≤—á–∞–Ω–Ω—è")
            speak_async("–ù–µ –≤–¥–∞–ª–æ—Å—è –∑–Ω–∞–π—Ç–∏ –æ–±–ª–∏—á—á—è –¥–ª—è –Ω–∞–≤—á–∞–Ω–Ω—è.")

        cv2.destroyAllWindows()
        return False

    except Exception as e:
        print(f"‚ùå –ü–æ–º–∏–ª–∫–∞ –Ω–∞–≤—á–∞–Ω–Ω—è –æ–±–ª–∏—á—á—è: {e}")
        speak_async("–ü–æ–º–∏–ª–∫–∞ –ø—Ä–∏ –Ω–∞–≤—á–∞–Ω–Ω—ñ –æ–±–ª–∏—á—á—è.")
        cv2.destroyAllWindows()
        return False


def update_face_model(classifier, le, name, training_samples):
    try:

        if hasattr(le, 'classes_'):
            existing_classes = list(le.classes_)
            if name not in existing_classes:
                existing_classes.append(name)
                le.fit(existing_classes)
        else:
            le.fit([name])

        encoded_labels = le.transform([name] * len(training_samples))
        classifier.fit(training_samples, encoded_labels)

        if save_face_data(classifier, le):
            print(f"‚úÖ –ú–æ–¥–µ–ª—å –æ–±–ª–∏—á –æ–Ω–æ–≤–ª–µ–Ω–æ –¥–ª—è {name}")
            return classifier, le
        else:
            print(f"‚ùå –ù–µ –≤–¥–∞–ª–æ—Å—è –æ–Ω–æ–≤–∏—Ç–∏ –º–æ–¥–µ–ª—å –¥–ª—è {name}")
            return None, None

    except Exception as e:
        print(f"‚ùå –ü–æ–º–∏–ª–∫–∞ –æ–Ω–æ–≤–ª–µ–Ω–Ω—è –º–æ–¥–µ–ª—ñ –æ–±–ª–∏—á: {e}")
        return None, None